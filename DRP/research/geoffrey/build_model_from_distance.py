#!/usr/bin/env python

#from DRP.models import PerformedReaction, ModelContainer, Descriptor, rxnDescriptorValues
from cPickle import load
from django.conf import settings
import importlib
import argparse
import numpy as np


def npToArff(writeable, data, labels, relationName='relation', headers=None, labelHeader=None):
    """Convert numpy array to arff"""
    writeable.write('%arff file generated by the Dark Reactions Project provided by Haverford College\n')
    writeable.write('%This arff is generated from a numpy array\n')
    writeable.write('\n@relation {}\n'.format(relationName))
    rows, columns = data.shape
    if headers is None:
        headers = ["descriptor_{}".format(i) for i in range(columns)]        
    if len(headers) != columns:
        raise ValueError("Headers must have the same length as the number of columns in the numpy array."
                         " There are {} columns and {} headers".format(columns, len(headers)))
    if labelHeader is None:
        labelHeader = 'outcome'
    labelArffHeader = '@attribute {} {{True, False}}'.format(labelHeader)
    arffheaders = ['@attribute {} numeric'.format(header) for header in headers]
    writeable.write('\n'.join(arffheaders))
    writeable.write('\n')
    writeable.write(labelArffHeader)
    writeable.write('\n\n@data\n')
    
    for row, label in zip(data, labels):
        writeable.write(','.join(['"'+str(item)+'"' for item in row] + ['"'+str(label)+'"']))
        writeable.write('\n')

def getMetricObject(metric_object_dump):
    with open(metric_object_dump, 'r') as f:
        metric_object_wrapper = load(f)

    return metric_object_wrapper

def getConfusionMatrix(predictionTuples):
        """
        Returns a dicionary of dictionaries, where the outer keys are the "correct" or "true"
       values, the inner keys are the "guessed" values that occurred, and
       the value is the integer number of occurrences of that guess when the
       true descriptor was the second key.

       Eg: {true: {guess:#, guess':#},
            true': {guess:#, guess':#}}
       Eg: {"1": {"1": 10
                  "2": 10
                  "3": 13
                  "4": 0
                 }
           , ...
           }
          }
        """
        matrix = {
                    True: {True: 0, False: 0},
                    False: {True: 0, False: 0}
                    }
        for true, guess in predictionTuples:
            matrix[true][guess] += 1

        return matrix

def stringToBool(s):
    if s == 'True':
        return True
    elif s == 'False':
        return False
    else:
        raise ValueError("Tried to convert string to boolean when string was neither 'True' nor 'False' but {}".format(outcome))


def build_model(modelVisitorLibrary, modelVisitorTool, metric_object_dump, model_file, train_arff_file, test_arff_file, out_file):

    visitorModules = {library:importlib.import_module(settings.STATS_MODEL_LIBS_DIR + "."+ library) for library in settings.STATS_MODEL_LIBS}
    modelVisitor = getattr(visitorModules[modelVisitorLibrary], modelVisitorTool)(None)

    metric_object_wrapper = getMetricObject(metric_object_dump)

    data = metric_object_wrapper.transform()
    labels = metric_object_wrapper.labels
    rows, columns = data.shape
    #labels.resize((labels.shape[0], 1))
    #X = np.concatenate((data, labels), axis=1)
    #rows, columns = X.shape
    train_data, test_data = np.split(data, [2*rows/3], axis=0)
    train_labels, test_labels = np.split(labels, [2*rows/3], axis=0)

    with open(train_arff_file, 'w') as f:
        npToArff(f, train_data, train_labels)
    with open(test_arff_file, 'w') as f:
        npToArff(f, test_data, test_labels)

    response_index = columns + 1
    modelVisitor.wekaTrain(train_arff_file, model_file, response_index)
    modelVisitor.wekaPredict(test_arff_file, model_file, response_index, out_file)

    predictions = modelVisitor._readWekaOutputFile(out_file, stringToBool)

    if len(predictions) != len(test_labels):
        raise ValueError("Predictions and test label lengths do not match. They are {} and {}, respectively".format(
                            len(predictions), len(test_labels)))

    predictionTuples = zip(test_labels, predictions)

    conf_mtrx = getConfusionMatrix(predictionTuples)
    print confusionMatrixString(conf_mtrx)
    print "Accuracy: {:.3}".format(accuracy(conf_mtrx))
    print "BCR: {:.3}".format(BCR(conf_mtrx))


#def display_model_results(container):
    #conf_mtrcs = container.getConfusionMatrices()

    #for model_mtrcs in conf_mtrcs:
        #for descriptor_header, conf_mtrx in model_mtrcs:
            #print "Confusion matrix for {}:".format(descriptor_header)
            #print confusionMatrixString(conf_mtrx)
            #print "Accuracy: {:.3}".format(accuracy(conf_mtrx))
            #print "BCR: {:.3}".format(BCR(conf_mtrx))


#def prepare_build_display_model(descriptor_headers, response_headers, modelVisitorLibrary, modelVisitorTool, splitter):
    #"""
    #Build and display a model with the specified tools
    #"""
    ## Grab all reactions with defined outcome descriptors
    #reactions = PerformedReaction.objects.all()
    #reactions = reactions.exclude(ordrxndescriptorvalue__in=rxnDescriptorValues.OrdRxnDescriptorValue.objects.filter(descriptor__heading__in=response_headers, value=None))
    #reactions = reactions.exclude(boolrxndescriptorvalue__in=rxnDescriptorValues.BoolRxnDescriptorValue.objects.filter(descriptor__heading__in=response_headers, value=None))
    #reactions = reactions.exclude(catrxndescriptorvalue__in=rxnDescriptorValues.CatRxnDescriptorValue.objects.filter(descriptor__heading__in=response_headers, value=None))

    #predictors = Descriptor.objects.filter(heading__in=descriptor_headers)
    #responses = Descriptor.objects.filter(heading__in=response_headers)

    #container = build_model(reactions, predictors, responses, modelVisitorLibrary, modelVisitorTool, splitter)

    #display_model_results(container)

def accuracy(conf):
    correct = 0.0
    total = 0.0
    for true, guesses in conf.items():
        for guess, count in guesses.items():
            if true == guess:
                correct += count
            total += count
    return correct/total

    
def BCR(conf):
    class_accuracy_sum = 0.0
    num_classes = 0.0
    for true, guesses in conf.items():
        num_classes += 1
        class_correct = 0.0
        class_total = 0.0
        for guess, count in guesses.items():
            if true == guess:
                class_correct += count
            class_total += count
        class_accuracy_sum += class_correct/class_total if class_total else 0.0
    
    return class_accuracy_sum/num_classes

def confusionMatrixString(confusionMatrix, headers=True):
    """
    Returns a string that will display a confusionMatrix
    If headers=True, includes the headers as the first row and first column.
    """

    table = confusionMatrixTable(confusionMatrix, headers)
    return ('\n'.join([''.join(['{:^6}'.format(item) for item in row]) for row in table]))


def confusionMatrixTable(confusionMatrix, headers=True):
    """
    Converts a confusion matrix dictionary to a list of lists.
    Primarily for display purposes.
    Each list corresponds to a single true value and contains the
    counts for each predicted value.
    If headers=True, includes the headers as the first row and first column.
    """

    values = confusionMatrix.keys()
    table = [[0 for predicted in values] for true in values]

    for i, true in enumerate(values):
        for j, predicted in enumerate(values):
            table[j][i] = confusionMatrix[true][predicted]

    if headers:
        for j, predicted in enumerate(values):
            table[j].insert(0, str(predicted))
        table.insert(0, [""] + map(str, values))

    return table

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Builds a model', fromfile_prefix_chars='@',
                                     epilog="Prefix arguments with '@' to specify a file containing newline"
                                     "-separated values for that argument. e.g.'-p @predictor_headers.txt'"
                                     " to pass multiple descriptors from a file as predictors")
    parser.add_argument('-t', '--test-arff')
    parser.add_argument('-T', '--train-arff')
    parser.add_argument('-ml', '--model-library', default="weka",
                        help='Model visitor library to use. (default: %(default)s)')
    parser.add_argument('-mt', '--model-tool', default="SVM_PUK_basic",
                        help='Model visitor tool from library to use. (default: %(default)s)')
    parser.add_argument('-o', '--weka-output-file')
    parser.add_argument('-mo', '--metric-object-dump')
    parser.add_argument('-mf', '--model-file')
    args = parser.parse_args()

    build_model(args.model_library, args.model_tool, args.metric_object_dump, args.model_file, args.train_arff, args.test_arff, args.weka_output_file)
